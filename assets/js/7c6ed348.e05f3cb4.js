"use strict";(self.webpackChunkapache_website_template=self.webpackChunkapache_website_template||[]).push([[4846],{7501:(e,r,a)=>{a.r(r),a.d(r,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var t=a(1527),n=a(395);const s={id:"spark",title:"Spark Library",sidebar_position:3},i=void 0,o={id:"libraries/spark/spark",title:"Spark Library",description:"Overview",source:"@site/docs/libraries/spark/spark.md",sourceDirName:"libraries/spark",slug:"/libraries/spark/",permalink:"/docs/libraries/spark/",draft:!1,unlisted:!1,editUrl:"https://github.com/apache/incubator-graphar/edit/main/docs/libraries/spark/spark.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{id:"spark",title:"Spark Library",sidebar_position:3},sidebar:"documentation",previous:{title:"How to Develop Java Library",permalink:"/docs/libraries/java/how_to_develop_java"},next:{title:"Examples",permalink:"/docs/libraries/spark/examples"}},l={},d=[{value:"Overview",id:"overview",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Get GraphAr Spark Library",id:"get-graphar-spark-library",level:2},{value:"Building from source",id:"building-from-source",level:3},{value:"How to Use",id:"how-to-use",level:2},{value:"Information classes",id:"information-classes",level:3},{value:"IndexGenerator",id:"indexgenerator",level:3},{value:"Writer",id:"writer",level:3},{value:"Reader",id:"reader",level:3},{value:"Graph-level APIs",id:"graph-level-apis",level:3},{value:"More examples",id:"more-examples",level:3},{value:"Working with Cloud Storage (AWS S3, aliyun OSS)",id:"working-with-cloud-storage-aws-s3-aliyun-oss",level:3}];function h(e){const r={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,n.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(r.p,{children:"The GraphAr Spark library is provided for generating, loading and transforming GraphAr format files with Apache Spark easy. It consists of several components:"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Information Classes"}),": As same with in the C++ library, the information classes are implemented as a part of the Spark library for constructing and accessing the meta information about the graphs, vertices and edges in GraphAr."]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"IndexGenerator"}),": The IndexGenerator helps to generate the indices for vertex/edge DataFrames. In most cases, IndexGenerator is first utilized to generate the indices for a DataFrame (e.g., from primary keys), and then this DataFrame can be written into GraphAr format files through the writer."]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Writer"}),": The GraphAr Spark writer provides a set of interfaces that can be used to write Spark DataFrames into GraphAr format files. Every time it takes a DataFrame as the logical table for a type of vertices or edges, assembles the data in specified format (e.g., reorganize the edges in the CSR way) and then dumps it to standard GraphAr format files (CSV, ORC or Parquet files) under the specific directory path."]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Reader"}),": The GraphAr Spark reader provides a set of interfaces that can be used to read GraphAr format files. It reads a collection of vertices or edges at a time and assembles the result into the Spark DataFrame. Similar with the reader in the C++ library, it supports the users to specify the data they need, e.g., reading a single property group instead of all properties."]}),"\n"]}),"\n",(0,t.jsx)(r.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,t.jsx)(r.p,{children:"The GraphAr Spark library can be used in a range of scenarios:"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Taking GraphAr format as a data source to execute SQL queries or do graph processing (e.g., using GraphX)."}),"\n",(0,t.jsx)(r.li,{children:"Transforming data between GraphAr format and other data sources (e.g., Hive, Neo4j, NebulaGraph, ...)."}),"\n",(0,t.jsx)(r.li,{children:"Transforming GraphAr format data between different file types (e.g., from ORC to Parquet)."}),"\n",(0,t.jsx)(r.li,{children:"Transforming GraphAr format data between different adjList types (e.g., from COO to CSR)."}),"\n",(0,t.jsx)(r.li,{children:"Modifying existing GraphAr format data (e.g., adding new vertices/edges)."}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:["For more information on its usage, please refer to the ",(0,t.jsx)(r.a,{href:"/docs/libraries/spark/examples",children:"Examples"}),"."]}),"\n",(0,t.jsx)(r.h2,{id:"get-graphar-spark-library",children:"Get GraphAr Spark Library"}),"\n",(0,t.jsx)(r.h3,{id:"building-from-source",children:"Building from source"}),"\n",(0,t.jsx)(r.p,{children:"Make the spark-library directory as the current working directory:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"cd incubator-graphar/maven_projects/spark/\n"})}),"\n",(0,t.jsx)(r.p,{children:"Compile package:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"mvn clean package -DskipTests\n"})}),"\n",(0,t.jsx)(r.p,{children:"GraphAr supports two Apache Spark versions for now and uses Maven Profiles to work with it. The command above built GraphAr with Spark 3.2.2 by default. To built GraphAr with Spark 3.3.4 use the following command:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"mvn clean package -DskipTests -P datasources-33\n"})}),"\n",(0,t.jsxs)(r.p,{children:["After compilation, a similar file ",(0,t.jsx)(r.em,{children:"graphar-x.x.x-SNAPSHOT-shaded.jar"})," is generated in the directory ",(0,t.jsx)(r.em,{children:"spark/graphar/target/"}),"."]}),"\n",(0,t.jsxs)(r.p,{children:["Please refer to the ",(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/tree/main/spark",children:"building steps"})," for more details."]}),"\n",(0,t.jsx)(r.h2,{id:"how-to-use",children:"How to Use"}),"\n",(0,t.jsx)(r.h3,{id:"information-classes",children:"Information classes"}),"\n",(0,t.jsx)(r.p,{children:"The Spark library for GraphAr provides distinct information classes for constructing and accessing meta information about graphs, vertices, and edges. These classes act as essential parameters for constructing readers and writers, and they can be built either from the existing meta files (in the Yaml format) or in-memory from scratch."}),"\n",(0,t.jsx)(r.p,{children:"To construct information from a Yaml file, please refer to the following example code."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-scala",children:"// read graph yaml and construct information\nval spark = ... // the Spark session\nval file_path = ... // the path to the yaml file\nval graph_info = GraphInfo.loadGraphInfo(file_path, spark)\n\n// use information classes\nval vertices = graph_info.getVertices\nval edges = graph_info.getEdges\nval version = graph_info.getVersion\n"})}),"\n",(0,t.jsxs)(r.p,{children:["See ",(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/TestGraphInfo.scala",children:"TestGraphInfo.scala"})," for the complete example."]}),"\n",(0,t.jsx)(r.h3,{id:"indexgenerator",children:"IndexGenerator"}),"\n",(0,t.jsx)(r.p,{children:'The GraphAr file format assigns each vertex with a unique index inside the vertex type (which called internal vertex id) starting from 0 and increasing continuously for each type of vertex (i.e., with the same vertex label). However, the vertex/edge tables in Spark often lack this information, requiring special attention. For example, an edge table typically uses the primary key (e.g., "id", which is a string) to identify its source and destination vertices.'}),"\n",(0,t.jsx)(r.p,{children:"To address this issue, the GraphAr Spark library offers the IndexGenerator which is used to generate indices for vertex/edge DataFrames. For a vertex DataFrame, a mapping from the primary keys to indices can be constructed, or an index column can be generated directly if no primary keys are available. For an edge DataFrame, source and destination columns can be generated from the vertex index mapping (when the end vertices are represented by the primary keys), or they may be generated directly without the mapping."}),"\n",(0,t.jsxs)(r.blockquote,{children:["\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"tip:"}),"\nIn most cases, IndexGenerator is first utilized to generate the indices for a DataFrame, and then this DataFrame can be written into GraphAr format files through the writer."]}),"\n"]}),"\n",(0,t.jsx)(r.p,{children:"To utilize IndexGenerator, please refer to the following example code."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-scala",children:'// generate indices for vertex DataFrame\nval vertex_df = ...\nval vertex_df_with_index = IndexGenerator.generateVertexIndexColumn(vertex_df)\n\n// generate indices for src & dst columns of edge DataFrame\nval edge_df = ...\nval edge_df_with_index = IndexGenerator.generateSrcAndDstIndexUnitedlyForEdges(edge_df, "src", "dst")\n\n// generate indices for src & dst columns of edge DataFrame from vertex primary keys\nval vertex_df = ...\nval edge_df = ...\nval vertex_mapping = IndexGenerator.constructVertexIndexMapping(vertex_df, "id")\nval edge_df_src_index = IndexGenerator.generateSrcIndexForEdgesFromMapping(edge_df, "src", vertex_mapping)\nval edge_df_src_dst_index = IndexGenerator.generateDstIndexForEdgesFromMapping(edge_df_src_index, "dst", vertex_mapping)\n'})}),"\n",(0,t.jsxs)(r.p,{children:["See ",(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/TestIndexGenerator.scala",children:"TestIndexGenerator.scala"})," for the complete example."]}),"\n",(0,t.jsx)(r.h3,{id:"writer",children:"Writer"}),"\n",(0,t.jsx)(r.p,{children:"The GraphAr Spark writer provides the necessary Spark interfaces to write DataFrames into GraphAr formatted files in a batch-import fashion. With the VertexWriter, users can specify a particular property group to be written into its corresponding chunks, or choose to write all property groups. For edge chunks, besides the meta data (edge info), the adjList type should also be specified. The adjList/properties can be written alone, or alternatively, all adjList, properties, and the offset (for CSR and CSC format) chunks can be written simultaneously."}),"\n",(0,t.jsx)(r.p,{children:"To utilize the GraphAr Spark writer, please refer to the following example code."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-scala",children:'// generate the vertex index column for vertex DataFrame\nval vertex_df = ...\nval vertex_df_with_index = IndexGenerator.generateVertexIndexColumn(vertex_df)\n// construct the vertex writer\nval vertex_info = ...\nval prefix = ...\nval writer = new VertexWriter(prefix, vertex_info, vertex_df_with_index)\n// write certain property group\nval property_group = vertex_info.getPropertyGroup("id")\nwriter.writeVertexProperties(property_group)\n// write all properties\nwriter.writeVertexProperties()\n\n// generate vertex index for edge DataFrame\nval edge_df = ...\nval edge_df_with_index = IndexGenerator.generateSrcAndDstIndexUnitedlyForEdges(edge_df, "src", "dst")\n// construct the edge writer\nval edge_info = ...\nval adj_list_type = AdjListType.ordered_by_source\nval writer = new EdgeWriter(prefix, edge_info, adj_list_type, edge_df_with_index)\n// write adjList\nwriter.writeAdjList()\n// write certain property group\nval property_group = edge_info.getPropertyGroup("creationDate", adj_list_type)\nwriter.writeEdgeProperties(property_group)\n// write all of adjList and properties\nwriter.writeEdges()\n'})}),"\n",(0,t.jsxs)(r.p,{children:["See ",(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/TestWriter.scala",children:"TestWriter.scala"})," for the complete example."]}),"\n",(0,t.jsx)(r.h3,{id:"reader",children:"Reader"}),"\n",(0,t.jsx)(r.p,{children:"The GraphAr Spark reader provides an extensive set of interfaces to read GraphAr format files. It reads a collection of vertices or edges at a time and assembles the result into the Spark DataFrame. Similar with the reader in C++ library, it supports the users to specify the data they need, e.g., a single property group."}),"\n",(0,t.jsx)(r.p,{children:"After content has been read into the Spark DataFrame, users can leverage it to do graph processing, execute SQL queries or perform various transformations (such as adding new vertices/edges, reorganizing the edge order, and changing the file type) and then write it back into GraphAr format files if desired."}),"\n",(0,t.jsx)(r.p,{children:"To utilize the GraphAr Spark reader, please refer to the following example code."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-scala",children:'// construct the vertex reader\nval prefix = ...\nval vertex_info = ...\nval reader = new VertexReader(prefix, vertex_info, spark)\nval property_group = vertex_info.getPropertyGroup("gender")\n// read a single chunk\nval single_chunk_df = reader.readVertexPropertyChunk(property_group, 0)\n// ...\n// read all property chunks\nval vertex_df = reader.readAllVertexPropertyGroups()\n\n//construct the edge reader\nval edge_info = ...\nval adj_list_type = AdjListType.ordered_by_source\nval reader = new EdgeReader(prefix, edge_info, adj_list_type, spark)\n// read a single adjList chunk\nval single_adj_list_df = reader.readAdjListChunk(2, 0)\n// read all adjList chunks for a vertex chunk\nval adj_list_df_chunk_2 = reader.readAdjListForVertexChunk(2)\n// ...\n// read all edge chunks (including adjList and all properties)\nval edge_df = reader.readEdges()\n'})}),"\n",(0,t.jsxs)(r.p,{children:["See ",(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/TestReader.scala",children:"TestReader.scala"})," for the complete example."]}),"\n",(0,t.jsx)(r.h3,{id:"graph-level-apis",children:"Graph-level APIs"}),"\n",(0,t.jsx)(r.p,{children:"To improve the usability of the GraphAr Spark library, a set of APIs are provided to allow users to easily perform operations such as reading, writing, and transforming data at the graph level. These APIs are fairly easy to use, while the previous methods of using reader, writer and information classes are more flexibly and can be highly customized."}),"\n",(0,t.jsxs)(r.p,{children:["The Graph Reader is a helper object which enables users to read all the chunk files from GraphAr for a single graph. The only input required is a GraphInfo object or the path to the information yaml file. On successful completion, it returns a set of vertex DataFrames and edge DataFrames, each of which can be accessed by specifying the vertex/edge label. The Graph Writer is used for writing all vertex DataFrames and edge DataFrames of a graph to generate GraphAr chunk files. For more details, please refer to the ",(0,t.jsx)(r.a,{href:"https://graphar.apache.org/docs/spark/",children:"API Reference"}),"."]}),"\n",(0,t.jsx)(r.p,{children:"The Graph Transformer is a helper object in the GraphAr Spark library, designed to assist with data transformation at the graph level. It takes two GraphInfo objects (or paths of two yaml files) as inputs: one for the source graph, and one for the destination graph. The transformer will then load data from existing GraphAr format files for the source graph, utilizing the GraphAr Spark Reader and the meta data defined in the source GraphInfo. After reorganizing the data according to the destination GraphInfo, it generates new GraphAr format chunk files with the GraphAr Spark Writer."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-scala",children:"// transform graphs by yaml paths\nval spark = ... // the Spark session\nval source_path = ... // e.g., /tmp/source.graph.yml\nval dest_path = ... // e.g., /tmp/dest.graph.yml\nGraphTransformer.transform(source_path, dest_path, spark)\n\n// transform graphs by information objects\nval source_info = ...\nval dest_info = ...\nGraphTransformer.transform(source_info, dest_info, spark)\n"})}),"\n",(0,t.jsxs)(r.p,{children:["We provide an example in ",(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/TestGraphTransformer.scala",children:"TestGraphTransformer.scala"}),", which demonstrates how to conduct data transformation from the ",(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar-testing/blob/main/ldbc_sample/parquet/ldbc_sample.graph.yml",children:"source graph"})," to the ",(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar-testing/blob/main/transformer/ldbc_sample.graph.yml",children:"destination graph"}),"."]}),"\n",(0,t.jsx)(r.p,{children:"The Graph Transformer can be used for various purposes, including transforming GraphAr format data between different file types (e.g. from ORC to Parquet), transforming between different adjList types (e.g. from COO to CSR), selecting properties or regrouping them, and setting a new chunk size."}),"\n",(0,t.jsxs)(r.admonition,{type:"note",children:[(0,t.jsx)(r.p,{children:"There are certain limitations while using the Graph Transformer:"}),(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:["\n",(0,t.jsx)(r.p,{children:"The vertices (or edges) of the source and destination graphs are aligned by labels, meaning each vertex/edge label included in the destination graph must have an equivalent in the source graph, in order for the related chunks to be loaded as the data source."}),"\n"]}),"\n",(0,t.jsxs)(r.li,{children:["\n",(0,t.jsx)(r.p,{children:"For each group of vertices/edges (i.e., each single label), each property included in the destination graph (defined in the relevant VertexInfo/EdgeInfo) must also be present in the source graph."}),"\n",(0,t.jsxs)(r.p,{children:["In addition, users can use the GraphAr Spark Reader/Writer to conduct data transformation more flexibly at the vertex/edge table level, as opposed to the graph level. This allows for a more granular approach to transforming data, as ",(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/TransformExample.scala",children:"TransformExample.scala"})," shows."]}),"\n"]}),"\n"]})]}),"\n",(0,t.jsx)(r.h3,{id:"more-examples",children:"More examples"}),"\n",(0,t.jsx)(r.p,{children:"For more information on usage, please refer to the examples:"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/ComputeExample.scala",children:"ComputeExample.scala"}),"  includes an example for constructing the GraphX graph from GraphAr format files and executing a connected-components computation."]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/TransformExample.scala",children:"TransformExample.scala"})," shows an example for graph data conversion between different file types or different adjList types."]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/main/scala/org/apache/graphar/example/Neo4j2GraphAr.scala",children:"Neo4j2GraphAr.scala"})," and ",(0,t.jsx)(r.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/main/scala/org/apache/graphar/example/GraphAr2Neo4j.scala",children:"GraphAr2Neo4j.scala"})," are examples to conduct data importing/exporting for Neo4j."]}),"\n"]}),"\n",(0,t.jsx)(r.h3,{id:"working-with-cloud-storage-aws-s3-aliyun-oss",children:"Working with Cloud Storage (AWS S3, aliyun OSS)"}),"\n",(0,t.jsx)(r.p,{children:"The Spark library for GraphAr supports reading and writing data from/to cloud storage services such as AWS S3, to do so, you need to include the Hadoop AWS dependency in your project. See the reference documentation for more details."}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:(0,t.jsx)(r.a,{href:"https://spark.apache.org/docs/latest/cloud-integration.html",children:"AWS S3"})}),"\n",(0,t.jsx)(r.li,{children:(0,t.jsx)(r.a,{href:"https://hadoop.apache.org/docs/stable/hadoop-aliyun/tools/hadoop-aliyun/index.html",children:"Aliyun OSS"})}),"\n"]})]})}function p(e={}){const{wrapper:r}={...(0,n.a)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},395:(e,r,a)=>{a.d(r,{Z:()=>o,a:()=>i});var t=a(959);const n={},s=t.createContext(n);function i(e){const r=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),t.createElement(s.Provider,{value:r},e.children)}}}]);