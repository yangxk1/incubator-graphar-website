"use strict";(self.webpackChunkapache_website_template=self.webpackChunkapache_website_template||[]).push([[6576],{9718:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>p,contentTitle:()=>t,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>l});var a=r(1527),o=r(395);const s={id:"how-to",title:"How to use GraphAr PySpark package",sidebar_position:1},t=void 0,i={id:"libraries/pyspark/how-to",title:"How to use GraphAr PySpark package",description:"GraphAr PySpark",source:"@site/docs/libraries/pyspark/how-to.md",sourceDirName:"libraries/pyspark",slug:"/libraries/pyspark/how-to",permalink:"/docs/libraries/pyspark/how-to",draft:!1,unlisted:!1,editUrl:"https://github.com/apache/incubator-graphar/edit/main/docs/libraries/pyspark/how-to.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{id:"how-to",title:"How to use GraphAr PySpark package",sidebar_position:1},sidebar:"documentation",previous:{title:"PySpark Library",permalink:"/docs/libraries/pyspark/"}},p={},l=[{value:"GraphAr PySpark",id:"graphar-pyspark",level:2},{value:"GraphAr PySpark initialize",id:"graphar-pyspark-initialize",level:2},{value:"GraphAr objects",id:"graphar-objects",level:2},{value:"Creating objects in graphar_pyspark",id:"creating-objects-in-graphar_pyspark",level:2},{value:"Loading Info objects from YAML",id:"loading-info-objects-from-yaml",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"graphar-pyspark",children:"GraphAr PySpark"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"graphar_pyspark"})," is implemented as bindings to GraphAr spark scala\nlibrary. You should have ",(0,a.jsx)(n.code,{children:"graphar-0.1.0-SNAPSHOT.jar"})," in your\nApache Spark JVM classpath. Otherwise you will get an exception. To\nadd it specify ",(0,a.jsx)(n.code,{children:'config("spark.jars", "path-to-graphar-jar")'})," when\nyou create a SparkSession:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession\n    .builder\n    .master("local[1]")\n    .appName("graphar-local-tests")\n    .config("spark.jars", "../../spark/graphar/target/graphar-0.1.0-SNAPSHOT.jar")\n    .config("spark.log.level", "INFO")\n    .getOrCreate()\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"graphar-pyspark-initialize",children:"GraphAr PySpark initialize"}),"\n",(0,a.jsxs)(n.p,{children:["PySpark bindings are heavily relying on JVM-calls via ",(0,a.jsx)(n.code,{children:"py4j"}),". To\ninitiate all the necessary things for it just call\n",(0,a.jsx)(n.code,{children:"graphar_pyspark.initialize()"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from graphar_pyspark import initialize\n\ninitialize(spark)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"graphar-objects",children:"GraphAr objects"}),"\n",(0,a.jsxs)(n.p,{children:["Now you can import, create and modify all the classes you can\ncall from ",(0,a.jsx)(n.a,{href:"https://graphar.apache.org/docs/spark/",children:"scala API of GraphAr"}),".\nFor simplify using of graphar from python constants, like GAR-types,\nsupported file-types, etc. are placed in ",(0,a.jsx)(n.code,{children:"graphar_pyspark.enums"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from graphar_pyspark.info import Property, PropertyGroup, AdjList, AdjListType, VertexInfo, EdgeInfo, GraphInfo\nfrom graphar_pyspark.enums import GarType, FileType\n"})}),"\n",(0,a.jsx)(n.p,{children:"Main objects of GraphAr are the following:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"GraphInfo"}),"\n",(0,a.jsx)(n.li,{children:"VertexInfo"}),"\n",(0,a.jsx)(n.li,{children:"EdgeInfo"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["You can check ",(0,a.jsx)(n.a,{href:"/docs/libraries/spark/",children:"Scala library documentation"}),"\nfor the more detailed information."]}),"\n",(0,a.jsx)(n.h2,{id:"creating-objects-in-graphar_pyspark",children:"Creating objects in graphar_pyspark"}),"\n",(0,a.jsxs)(n.p,{children:["GraphAr PySpark package provide two main ways how to initiate\nobjects, like ",(0,a.jsx)(n.code,{children:"GraphInfo"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"from_python(**args)"})," when you create an object based on\npython-arguments"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"from_scala(jvm_ref)"})," when you create an object from the\ncorresponded JVM-object (",(0,a.jsx)(n.code,{children:"py4j.java_gateway.JavaObject"}),")"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"help(Property.from_python)\n\nHelp on method from_python in module graphar_pyspark.info:\n\nfrom_python(name: 'str', data_type: 'GarType', is_primary: 'bool') -> 'PropertyType' method of builtins.type instance\n       Create an instance of the Class from Python arguments.\n       \n       :param name: property name\n       :param data_type: property data type\n       :param is_primary: flag that property is primary\n       :returns: instance of Python Class.\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"python_property = Property.from_python(name=\"my_property\", data_type=GarType.INT64, is_primary=False)\nprint(type(python_property))\n\n<class 'graphar_pyspark.info.Property'>\n"})}),"\n",(0,a.jsxs)(n.p,{children:["You can always get a reference to the corresponding JVM object. For\nexample, if you want to use it in your own code and need a direct link\nto the underlying instance of Scala Class, you can just call\n",(0,a.jsx)(n.code,{children:"to_scala()"})," method:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"scala_obj = python_property.to_scala()\nprint(type(scala_obj))\n\n<class 'py4j.java_gateway.JavaObject'>\n"})}),"\n",(0,a.jsx)(n.p,{children:"As we already mentioned, you can initialize an instance of the Python\nclass from the JVM object:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"help(Property.from_scala)\n\nHelp on method from_scala in module graphar_pyspark.info:\n\n   from_scala(jvm_obj: 'JavaObject') -> 'PropertyType' method of builtins.type instance\n       Create an instance of the Class from the corresponding JVM object.\n       \n       :param jvm_obj: scala object in JVM.\n       :returns: instance of Python Class.\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"python_property = Property.from_scala(scala_obj)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Each public property and method of the Scala API is provided in\npython, but in a pythonic-naming convention. For example, in Scala,\n",(0,a.jsx)(n.code,{children:"Property"})," has the following fields:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"name"}),"\n",(0,a.jsx)(n.li,{children:"data_type"}),"\n",(0,a.jsx)(n.li,{children:"is_primary"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"For each of such a field in Scala API there is a getter and setter\nmethods. You can call them from the Python too:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"python_property.get_name()\n\n'my_property'\n"})}),"\n",(0,a.jsx)(n.p,{children:"You can also modify fields, but be careful: when you modify field of\ninstance of the Python class, you modify the underlying Scala Object\nat the same moment!"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"new_name = \"my_renamed_property\"\npython_property.set_name(new_name)\npython_property.get_name()\n\n'my_renamed_property'\n"})}),"\n",(0,a.jsx)(n.h2,{id:"loading-info-objects-from-yaml",children:"Loading Info objects from YAML"}),"\n",(0,a.jsxs)(n.p,{children:["But manual creating of objects is not a primary way of using GraphAr\nPySpark. ",(0,a.jsx)(n.code,{children:"GraphInfo"}),", ",(0,a.jsx)(n.code,{children:"VertexInfo"})," and ",(0,a.jsx)(n.code,{children:"EdgeInfo"})," can be also\ninitialized by reading from YAML-files:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'modern_graph_v_person = VertexInfo.load_vertex_info("../../testing/modern_graph/person.vertex.yml")\nmodern_graph_e_person_person = EdgeInfo.load_edge_info("../../testing/modern_graph/person_knows_person.edge.yml")\nmodern_graph = GraphInfo.load_graph_info("../../testing/modern_graph/modern_graph.graph.yml")\n'})}),"\n",(0,a.jsx)(n.p,{children:"After that you can work with such an objects like regular python\nobjects:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'print(modern_graph_v_person.dump())\n\n"\nchunk_size: 2\nprefix: vertex/person/\nproperty_groups:\n  - prefix: id/\n    file_type: csv\n    properties:\n      - is_primary: true\n        name: id\n        data_type: int64\n  - prefix: name_age/\n    file_type: csv\n    properties:\n      - is_primary: false\n        name: name\n        data_type: string\n      - is_primary: false\n        name: age\n        data_type: int64\nlabel: person\nversion: gar/v1\n"      \n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'print(modern_graph_v_person.contain_property("id") is True)\nprint(modern_graph_v_person.contain_property("bad_id?") is False)\n            \nTrue\nTrue\n'})}),"\n",(0,a.jsx)(n.p,{children:"Please, refer to Scala API and examples of GraphAr Spark Scala\nlibrary to see detailed and business-case oriented examples!"})]})}function h(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},395:(e,n,r)=>{r.d(n,{Z:()=>i,a:()=>t});var a=r(959);const o={},s=a.createContext(o);function t(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);