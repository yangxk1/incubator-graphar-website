"use strict";(self.webpackChunkapache_website_template=self.webpackChunkapache_website_template||[]).push([[7299],{2553:(e,a,r)=>{r.r(a),r.d(a,{assets:()=>p,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>h});var t=r(1527),n=r(395);const o={id:"examples",title:"Examples",sidebar_position:1},i=void 0,s={id:"libraries/spark/examples",title:"Examples",description:"Co-Work with Apache Spark",source:"@site/docs/libraries/spark/examples.md",sourceDirName:"libraries/spark",slug:"/libraries/spark/examples",permalink:"/docs/libraries/spark/examples",draft:!1,unlisted:!1,editUrl:"https://github.com/apache/incubator-graphar/edit/main/docs/libraries/spark/examples.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{id:"examples",title:"Examples",sidebar_position:1},sidebar:"documentation",previous:{title:"Spark Library",permalink:"/docs/libraries/spark/"},next:{title:"PySpark Library",permalink:"/docs/libraries/pyspark/"}},p={},h=[{value:"Co-Work with Apache Spark",id:"co-work-with-apache-spark",level:2},{value:"Examples",id:"examples",level:3},{value:"Transform GraphAr format files",id:"transform-graphar-format-files",level:3},{value:"Compute with GraphX",id:"compute-with-graphx",level:3},{value:"Import/Export graphs of Neo4j",id:"importexport-graphs-of-neo4j",level:3}];function l(e){const a={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,n.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.h2,{id:"co-work-with-apache-spark",children:"Co-Work with Apache Spark"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"https://spark.apache.org/",children:"Apache Spark"})," is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters. The GraphAr Spark library is developed to make the integration of GraphAr with Spark easy. This library allows users to efficiently generate, load, and transform GraphAr format files, and to integrate GraphAr with other Spark-compatible systems."]}),"\n",(0,t.jsx)(a.p,{children:"Examples of this co-working integration have been provided as showcases."}),"\n",(0,t.jsx)(a.h3,{id:"examples",children:"Examples"}),"\n",(0,t.jsx)(a.h3,{id:"transform-graphar-format-files",children:"Transform GraphAr format files"}),"\n",(0,t.jsxs)(a.p,{children:["We provide an example in ",(0,t.jsx)(a.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/TestGraphTransformer.scala",children:"TestGraphTransformer.scala"}),", which demonstrates\nhow to conduct data transformation at the graph level. ",(0,t.jsx)(a.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/TransformExample.scala",children:"TransformExample.scala"})," is\nanother example for graph data conversion between different file types or different\nadjList types, which is implemented at the vertex/edge table level. To do this,\nthe original data is first loaded into a Spark DataFrame using the GraphAr Spark Reader.\nThen, the DataFrame is written into generated GraphAr format files through a GraphAr Spark Writer,\nfollowing the meta data defined in a new information file."]}),"\n",(0,t.jsx)(a.h3,{id:"compute-with-graphx",children:"Compute with GraphX"}),"\n",(0,t.jsxs)(a.p,{children:["Another important use case of GraphAr is to use it as a data source for graph\ncomputing or analytics; ",(0,t.jsx)(a.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/test/scala/org/apache/graphar/ComputeExample.scala",children:"ComputeExample.scala"})," provides an example of constructing\na GraphX graph from reading GraphAr format files and executing a connected-components computation.\nAlso, executing queries with Spark SQL and running other graph analytic algorithms\ncan be implemented in a similar fashion."]}),"\n",(0,t.jsx)(a.h3,{id:"importexport-graphs-of-neo4j",children:"Import/Export graphs of Neo4j"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"https://neo4j.com/product/neo4j-graph-database",children:"Neo4j"})," graph database provides\na ",(0,t.jsx)(a.a,{href:"https://neo4j.com/docs/spark/current/overview/",children:"spark connector"})," for integration\nwith Spark. The Neo4j Spark Connector, combined with the GraphAr Spark library,\nenables us to migrate graph data between Neo4j and GraphAr. This is also a key application\nof GraphAr in which it acts as a persistent storage of graph data in the database."]}),"\n",(0,t.jsxs)(a.p,{children:["We provide two example programs that demonstrate how GraphAr can be used in conjunction\nwith Neo4j. It utilizes one of the built-in Neo4j datasets, the ",(0,t.jsx)(a.a,{href:"https://neo4j.com/developer/example-data/#built-in-examples",children:"Movie Graph"}),",\nwhich is a mini graph application containing actors and directors that are related through the movies they have collaborated on.\nGiven some necessary information like the chunk size, the prefix of the file path, and the file type,\nthe program can read the graph data from Neo4j and write it into GraphAr files.\nWhen exporting graph data from Neo4j and writing to GraphAr, please refer to the following code,\nwith ",(0,t.jsx)(a.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/main/scala/org/apache/graphar/example/Neo4j2GraphAr.scala",children:"Neo4j2GraphAr.scala"})," providing a complete example."]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'def main(args: Array[String]): Unit = {\n    // connect to the Neo4j instance\n    val spark = SparkSession.builder()\n      .appName("Neo4j to GraphAr for Movie Graph")\n      .config("neo4j.url", "bolt://localhost:7687")\n      .config("neo4j.authentication.type", "basic")\n      .config("neo4j.authentication.basic.username", sys.env.get("Neo4j_USR").get)\n      .config("neo4j.authentication.basic.password", sys.env.get("Neo4j_PWD").get)\n      .config("spark.master", "local")\n      .getOrCreate()\n\n    // initialize a graph writer\n    val writer: GraphWriter = new GraphWriter()\n\n    // put movie graph data into writer\n    readAndPutDataIntoWriter(writer, spark)\n\n    // write in GraphAr format\n    val outputPath: String = args(0)\n    val vertexChunkSize: Long = args(1).toLong\n    val edgeChunkSize: Long = args(2).toLong\n    val fileType: String = args(3)\n\n    writer.write(outputPath, spark, "MovieGraph", vertexChunkSize, edgeChunkSize, fileType)\n}\n'})}),"\n",(0,t.jsxs)(a.p,{children:["The ",(0,t.jsx)(a.code,{children:"readAndPutDataIntoWriter"})," method read the vertex and edge from Neo4j to DataFrame\n(Refer to ",(0,t.jsx)(a.a,{href:"https://neo4j.com/docs/spark/current/reading/",children:"Neo4j docs"})," for more details),\nand then put then into a GraphAr GraphWriter."]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'def readAndPutDataIntoWriter(writer: GraphWriter, spark: SparkSession): Unit = {\n  // read vertices with label "Person" from Neo4j as a DataFrame\n  val person_df = spark.read.format("org.neo4j.spark.DataSource")\n    .option("query", "MATCH (n:Person) RETURN n.name AS name, n.born as born")\n    .load()\n  // put into writer, vertex label is "Person"\n  writer.PutVertexData("Person", person_df)\n\n  // read vertices with label "Movie" from Neo4j as a DataFrame\n  val movie_df = spark.read.format("org.neo4j.spark.DataSource")\n    .option("query", "MATCH (n:Movie) RETURN n.title AS title, n.tagline as tagline")\n    .load()\n  // put into writer, vertex label is "Movie"\n  writer.PutVertexData("Movie", movie_df)\n\n  // read edges with type "Person"->"PRODUCED"->"Movie" from Neo4j as a DataFrame\n  val produced_edge_df = spark.read.format("org.neo4j.spark.DataSource")\n    .option("query", "MATCH (a:Person)-[r:PRODUCED]->(b:Movie) return a.name as src, b.title as dst")\n    .load()\n  // put into writer, source vertex label is "Person", edge label is "PRODUCED"\n  // target vertex label is "Movie"\n  writer.PutEdgeData(("Person", "PRODUCED", "Movie"), produced_edge_df)\n'})}),"\n",(0,t.jsxs)(a.p,{children:["Finally, the ",(0,t.jsx)(a.code,{children:"write"})," method writes the graph data in GraphAr format to the specified path."]}),"\n",(0,t.jsx)(a.p,{children:"Additionally, when importing data from GraphAr to create/update instances in Neo4j, please refer to the following code:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'def main(args: Array[String]): Unit = {\n    // connect to the Neo4j instance\n    val spark = SparkSession.builder()\n      .appName("GraphAr to Neo4j for Movie Graph")\n      .config("neo4j.url", "bolt://localhost:7687")\n      .config("neo4j.authentication.type", "basic")\n      .config("neo4j.authentication.basic.username", sys.env.get("Neo4j_USR").get)\n      .config("neo4j.authentication.basic.password", sys.env.get("Neo4j_PWD").get)\n      .config("spark.master", "local")\n      .getOrCreate()\n\n    // path to the graph information file\n    val graphInfoPath: String = args(0)\n    val graphInfo = GraphInfo.loadGraphInfo(graphInfoPath, spark)\n\n    val graphData = GraphReader.read(graphInfoPath, spark)\n    val vertexData = graphData._1\n    val edgeData = graphData._2\n\n    putVertexDataIntoNeo4j(graphInfo, vertexData, spark)\n    putEdgeDataIntoNeo4j(graphInfo, vertexData, edgeData, spark)\n}\n'})}),"\n",(0,t.jsxs)(a.p,{children:["Pass the graph information file path to ",(0,t.jsx)(a.code,{children:"loadGraphInfo"})," method to get the graph information.\nThen, read the graph data from GraphAr files with ",(0,t.jsx)(a.code,{children:"GraphReader"})," as DataFrame pair,\n",(0,t.jsx)(a.code,{children:"_1"})," for vertices and ",(0,t.jsx)(a.code,{children:"_2"})," for edges."]}),"\n",(0,t.jsxs)(a.p,{children:["The ",(0,t.jsx)(a.code,{children:"putVertexDataIntoNeo4j"})," and ",(0,t.jsx)(a.code,{children:"putEdgeDataIntoNeo4j"})," methods creates or updates the vertices DataFrame and edges DataFrame in Neo4j."]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'def putVertexDataIntoNeo4j(graphInfo: GraphInfo,\n                           vertexData: Map[String, DataFrame],\n                           spark: SparkSession): Unit = {\n  // write each vertex type to Neo4j\n  vertexData.foreach { case (key, df) => {\n    val primaryKey = graphInfo.getVertexInfo(key).getPrimaryKey()\n    // the vertex index column is not needed in Neo4j\n    // write to Neo4j, refer to https://neo4j.com/docs/spark/current/writing/\n    df.drop(GeneralParams.vertexIndexCol).write.format("org.neo4j.spark.DataSource")\n      .mode(SaveMode.Overwrite)\n      .option("labels", ":" + key)\n      .option("node.keys", primaryKey)\n      .save()\n  }}\n}\n\ndef putEdgeDataIntoNeo4j(graphInfo: GraphInfo,\n                         vertexData: Map[String, DataFrame],\n                         edgeData: Map[(String, String, String), Map[String, DataFrame]],\n                         spark: SparkSession): Unit = {\n  // write each edge type to Neo4j\n  edgeData.foreach { case (key, value) => {\n    // key is (source vertex label, edge label, target vertex label)\n    val sourceLabel = key._1\n    val edgeLabel = key._2\n    val targetLabel = key._3\n    val sourcePrimaryKey = graphInfo.getVertexInfo(sourceLabel).getPrimaryKey()\n    val targetPrimaryKey = graphInfo.getVertexInfo(targetLabel).getPrimaryKey()\n    val sourceDf = vertexData(sourceLabel)\n    val targetDf = vertexData(targetLabel)\n    // convert the source and target index column to the primary key column\n    val df = Utils.joinEdgesWithVertexPrimaryKey(value.head._2, sourceDf, targetDf, sourcePrimaryKey, targetPrimaryKey)  // use the first DataFrame of (adj_list_type_str, DataFrame) map\n\n    val properties = if (edgeLabel == "REVIEWED") "rating,summary" else ""\n\n    // write to Neo4j, refer to https://neo4j.com/docs/spark/current/writing/\n    df.write.format("org.neo4j.spark.DataSource")\n      .mode(SaveMode.Overwrite)\n      .option("relationship", edgeLabel)\n      .option("relationship.save.strategy", "keys")\n      .option("relationship.source.labels", ":" + sourceLabel)\n      .option("relationship.source.save.mode", "match")\n      .option("relationship.source.node.keys", "src:" + sourcePrimaryKey)\n      .option("relationship.target.labels", ":" + targetLabel)\n      .option("relationship.target.save.mode", "match")\n      .option("relationship.target.node.keys", "dst:" + targetPrimaryKey)\n      .option("relationship.properties", properties)\n      .save()\n  }}\n}\n'})}),"\n",(0,t.jsx)(a.p,{children:"Finally, you will see the graph in Neo4j Browser after running the above code."}),"\n",(0,t.jsxs)(a.p,{children:["See ",(0,t.jsx)(a.a,{href:"https://github.com/apache/incubator-graphar/blob/main/maven-projects/spark/graphar/src/main/scala/org/apache/graphar/example/GraphAr2Neo4j.scala",children:"GraphAr2Neo4j.scala"})," for the complete example."]}),"\n",(0,t.jsx)(a.admonition,{type:"tip",children:(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsxs)(a.li,{children:["The Neo4j Spark Connector offers different save modes and writing options, such as Append(CREATE) or Overwrite(MERGE). Please refer to its ",(0,t.jsx)(a.a,{href:"https://neo4j.com/docs/spark/current/writing/",children:"documentation"})," for more information and take the most appropriate method while using."]}),"\n",(0,t.jsxs)(a.li,{children:["The Neo4j Spark Connector supports to use ",(0,t.jsx)(a.a,{href:"https://neo4j.com/docs/spark/current/streaming",children:"Spark structured streaming API"}),", which works differently from Spark batching. One can utilize this API to read/write a stream from/to Neo4j, avoiding to maintain all data in the memory."]}),"\n"]})})]})}function c(e={}){const{wrapper:a}={...(0,n.a)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},395:(e,a,r)=>{r.d(a,{Z:()=>s,a:()=>i});var t=r(959);const n={},o=t.createContext(n);function i(e){const a=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function s(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),t.createElement(o.Provider,{value:a},e.children)}}}]);