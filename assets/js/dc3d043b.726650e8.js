"use strict";(self.webpackChunkapache_website_template=self.webpackChunkapache_website_template||[]).push([[7178],{7947:(e,r,a)=>{a.r(r),a.d(r,{assets:()=>p,contentTitle:()=>s,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>l});var i=a(1527),n=a(395);const t={id:"pyspark",title:"PySpark Library",sidebar_position:4},s=void 0,o={id:"libraries/pyspark/pyspark",title:"PySpark Library",description:"The current policy of GraphAr project is that for Apache Spark",source:"@site/docs/libraries/pyspark/pyspark.md",sourceDirName:"libraries/pyspark",slug:"/libraries/pyspark/",permalink:"/docs/libraries/pyspark/",draft:!1,unlisted:!1,editUrl:"https://github.com/apache/incubator-graphar/edit/main/docs/libraries/pyspark/pyspark.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{id:"pyspark",title:"PySpark Library",sidebar_position:4},sidebar:"documentation",previous:{title:"Examples",permalink:"/docs/libraries/spark/examples"},next:{title:"How to use GraphAr PySpark package",permalink:"/docs/libraries/pyspark/how-to"}},p={},l=[{value:"Overview",id:"overview",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Get GraphAr Spark Library",id:"get-graphar-spark-library",level:2},{value:"Building from source",id:"building-from-source",level:3},{value:"Get from PyPI",id:"get-from-pypi",level:3},{value:"How to Use",id:"how-to-use",level:2},{value:"Initialization",id:"initialization",level:3}];function d(e){const r={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,n.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.admonition,{type:"note",children:(0,i.jsxs)(r.p,{children:["The current policy of GraphAr project is that for Apache Spark\nthe main API is Scala Spark API. PySpark API follows scala Spark API.\nPlease refer to ",(0,i.jsx)(r.a,{href:"/docs/libraries/spark/",children:"GraphAr Spark library"}),"\nfor more detailed information about how to use GraphAr with Apache\nSpark."]})}),"\n",(0,i.jsx)(r.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(r.p,{children:"The GraphAr PySpark library is provided for generating, loading and\ntransforming GraphAr format files with PySpark."}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Information Classes"}),": As same with in the C++ library, the\ninformation classes are implemented as a part of the PySpark library\nfor constructing and accessing the meta information about the graphs,\nvertices and edges in GraphAr."]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"IndexGenerator"}),": The IndexGenerator helps to generate the indices\nfor vertex/edge DataFrames. In most cases, IndexGenerator is first\nutilized to generate the indices for a DataFrame (e.g., from primary\nkeys), and then this DataFrame can be written into GraphAr format files through\nthe writer."]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Writer"}),": The GraphAr PySpark writer provides a set of interfaces\nthat can be used to write Spark DataFrames into GraphAr format files. Every time\nit takes a DataFrame as the logical table for a type of vertices or\nedges, assembles the data in specified format (e.g., reorganize the\nedges in the CSR way) and then dumps it to standard GraphAr format files (CSV,\nORC or Parquet files) under the specific directory path."]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Reader"}),": The GraphAr PySpark reader provides a set of interfaces\nthat can be used to read GraphAr format files. It reads a collection of vertices\nor edges at a time and assembles the result into the Spark DataFrame.\nSimilar with the reader in the C++ library, it supports the users to\nspecify the data they need, e.g., reading a single property group\ninstead of all properties."]}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,i.jsx)(r.p,{children:"The GraphAr Spark library can be used in a range of scenarios:"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Taking GraphAr format data as a data source to execute SQL queries or do graph\nprocessing (e.g., using GraphX)."}),"\n",(0,i.jsx)(r.li,{children:"Transforming data between GraphAr format data and other data sources (e.g., Hive,\nNeo4j, NebulaGraph, \u2026)."}),"\n",(0,i.jsx)(r.li,{children:"Transforming GraphAr format data between different file types (e.g., from ORC to\nParquet)."}),"\n",(0,i.jsx)(r.li,{children:"Transforming GraphAr format data between different adjList types (e.g., from COO\nto CSR)."}),"\n",(0,i.jsx)(r.li,{children:"Modifying existing GraphAr format data (e.g., adding new vertices/edges)."}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"get-graphar-spark-library",children:"Get GraphAr Spark Library"}),"\n",(0,i.jsx)(r.h3,{id:"building-from-source",children:"Building from source"}),"\n",(0,i.jsxs)(r.p,{children:["GraphAr PySpark uses poetry as a build system. Please refer to\n",(0,i.jsx)(r.a,{href:"https://python-poetry.org/docs/#installation",children:"Poetry documentation"}),"\nto find the manual how to install this tool. Currently GraphAr PySpark\nis build with Python 3.9 and PySpark 3.2"]}),"\n",(0,i.jsx)(r.p,{children:"Make the graphar-pyspark-library directory as the current working\ndirectory:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"cd incubator-graphar/pyspark\n"})}),"\n",(0,i.jsx)(r.p,{children:"Build package:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"poetry build\n"})}),"\n",(0,i.jsxs)(r.p,{children:["After compilation, a similar file ",(0,i.jsx)(r.em,{children:"graphar_pyspark-0.0.1.tar.gz"})," is\ngenerated in the directory ",(0,i.jsx)(r.em,{children:"pyspark/dist/"}),"."]}),"\n",(0,i.jsx)(r.h3,{id:"get-from-pypi",children:"Get from PyPI"}),"\n",(0,i.jsx)(r.p,{children:"You cannot install graphar-pyspark from PyPi for now."}),"\n",(0,i.jsx)(r.h2,{id:"how-to-use",children:"How to Use"}),"\n",(0,i.jsx)(r.h3,{id:"initialization",children:"Initialization"}),"\n",(0,i.jsxs)(r.p,{children:["GraphAr PySpark is not a standalone library but bindings to GraphAr\nScala. You need to have ",(0,i.jsx)(r.em,{children:"spark-x.x.x.jar"})," in your ",(0,i.jsx)(r.em,{children:"spark-jars"}),".\nPlease refer to ",(0,i.jsx)(r.a,{href:"/docs/libraries/spark/",children:"GraphAr scala documentation"})," to get\nthis JAR."]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"// create a SparkSession from pyspark.sql import SparkSession\n\nspark = ( SparkSession .builder. \u2026 .conf(\u201cspark-jars\u201d,\n\u201cpath-to-graphar-spark-x.x.x.jar-file\u201d) .getOrCreate() )\n\nfrom graphar_pyspark import initialize initialize(spark)\n"})}),"\n",(0,i.jsx)(r.p,{children:"After initialization you can use the same API like in GraphAr scala\nlibrary."})]})}function h(e={}){const{wrapper:r}={...(0,n.a)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},395:(e,r,a)=>{a.d(r,{Z:()=>o,a:()=>s});var i=a(959);const n={},t=i.createContext(n);function s(e){const r=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),i.createElement(t.Provider,{value:r},e.children)}}}]);